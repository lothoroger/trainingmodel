{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 35px;\">Embedding a Machine Learning Model into a Flask Web Application</h1><hr style=\"border: 1px solid #f00\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must have this install through _pip_ or _conda_:\n",
    "* jupyter notebook\n",
    "* numpy\n",
    "* scikit-learn\n",
    "* nltk\n",
    "* Flask\n",
    "* WTForms\n",
    "* re\n",
    "* PyPrind\n",
    "* Pickle\n",
    "* SQLite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training a model for movie review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is we are training a logistic regression model for movie review classification, execute the following code blocks to train a model that we will serialize in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world applications we will be working with large datasets that may exceed our computer's memory. Not everyone has access to a supercomputer, so we have to apply a technique called _out-of-core learning_ that allows us to work with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of **stochastic gradient descent** is an optimization algorithm that updates the model's weights using one sample at a time. In this section, we will make use of the **partial_fit** function of the **SGDClassifier** in scikit-learn to stream the documents directly from our local drive and train a logistic regression model using small minibatches of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code example above, we define a **tokenizer** function that cleans the unprocessed text data from **movie_data.csv**. This will separate it into word tokens while removing stop words.\n",
    "\n",
    "Next we define a generator function called **steam_docs**, which reads in and returns one document at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that **steam_docs** function works correctly, we will read the first document from **movie_data.csv** file. This will return a tuple of the review text as well as the corresponding class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a function called **get_minibatch** that will take a document stream from the **stream_docs** function and return a particular number of documents specified by the _size_ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **HashingVectorizer** from scikit-learn, which is a useful vectorizer for text processing.\n",
    "\n",
    "**HashingVectorizer** is data-independent and makes use of the Hashing trick via the 32-bit MurmurHash3 algorithm by Austin Appleby(https://sites.google.com/site/murmurhash/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we intialized **HashingVectorizer** with our **tokenizer** function and set the number of features to 2<sup>21</sup>. Then we reinitialized a logistic regression classifier by setting the _loss_ parameter of the **SGDClassifier** to _log_, by choosing a large number of features in the **HashingVectorizer**, which reduced the chance of cause hash collisions, but we also increased the number of coeffients in our logistic regression model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up all the complementary functions, we now start the _out-of-core learning_ using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the PyPrind package in order to estimate the progress of our learning algorithm. We initialized the progress bar object with 45 iterations and in the following _for_ loop, we iterated over 45 minibatches of documents where each minibatch consists of 1,000 documents each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[#####                         ] | ETA: 00:01:18"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the incremental learning process, we will use the last 5,000 documents to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is 87 percent, which is not bad. We used the _out-of-core learning_ algorithm, which is very memory-effient and took less than a minute to complete. Finally, we can use the last 5,000 documents to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Serializing fitted scikit-learn estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the logistic regression model as shown last section, we can now save the classifier along with the stop words, Porter Stemmer, and `HashingVectorizer` as serialized objects to our local disk so that we can use the fitted classifier in our web application later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a machine learning model can be computationally quite expensive, as we have seen in the last section, Applying Machine Learning to Sentiment Analysis. Surely, we don't want to train our model every time we close our Python interpreter and want to make a new prediction or reload our web application? One option for model persistence is Python's in-built pickle module (https://docs.python.org/2.7/library/pickle.html), which allows us to serialize and de-serialize Python object structures to compact byte code, so that we can save our classifier in its current state and reload it if we want to classify new samples without needing to learn the model from the training data all over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we created a _movieclassifier_ directory where we will later store the files and data for our web application. Within this _movieclassifier_ directory, we created a **pkl_objects** subdirectory to save the serialized Python objects to our local drive. Via pickle's **dump** method, we then serialized the trained logistic regression model as well as the stop word set from the NLTK library so that we don't have to install the NLTK vocabulary on our server. The **dump** method takes as its first argument the object that we want to pickle, and for the second argument we provided an open file object that the Python object will be written to. Via the wb argument inside the open function, we opened the file in binary mode for pickle, and we set _protocol=2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=2)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the `HashingVectorizer` as in a separate file so that we can import it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the preceeding code cells, we can now restart the IPython notebook kernel to check if the objects were serialized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, change the current Python directory to `movieclassifer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 82.53%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]], clf.predict_proba(X).max()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a SQLite database for data storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute this code, please make sure that you are currently in the `movieclassifier` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM review_db WHERE date BETWEEN '2015-01-01 10:10:10' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'I love this movie', 1, u'2016-04-21 23:26:57'), (u'I disliked this movie', 0, u'2016-04-21 23:26:57')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a web application with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Flask library if its not in your current Python environment. \n",
    "\n",
    "```python\n",
    "pip install flask\n",
    "```\n",
    "or\n",
    "```python\n",
    "conda install flask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first Flask web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a directory tree like this example in your current directory:\n",
    "\n",
    "```html\n",
    "flask_app_1/\n",
    "    app.py\n",
    "    templates/\n",
    "        first_app.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **app.py** file will contain the main code that will be executed. The _templates_ directory is the directory that will hold all your static HTML, CSS, & JavaScript files. \n",
    "\n",
    "Let's now create the **app.py** inside your flask_app_1 directory by creating a Textfile in jupyter notebook and renaming it to **app.py**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from flask import Flask, render_template\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('first_app.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we running our application as a single module, initializing a new Flask instance with argument **__name__** to let Flask know that it can find the HTML template folder (_templates_) in the same directory. Then we used the **@app.route('/')** decorator to specify URL that will trigger the execution of the **index()** function. Now, the **index()** function will render the **first_app.html** file. Then we used the **if __name __ == '__main__':** statement to tell the Python interpreter to run the **app.run()** function on the server when this script is directly executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add some HTML content on the **first_app.html** file:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>First App</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>Hi, this my first Flask web app!</div>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start our web application by executing the command from the terminal or command prompt inside the flask_app_1 directory:\n",
    "\n",
    "```html\n",
    "python app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see a line with following displayed in on your terminal or command prompt:\n",
    "```html\n",
    "* Running on http://127.0.0.1:5000/\n",
    "```\n",
    "This line contains the address your local server. Just enter the address in your web browser to see the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form validation and rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next app, we will take simple Flask web application with HTML form elements to learn how to collect data from a user using the **WTForms** library (https://wtforms.readthedocs.org/en/latest/).\n",
    "\n",
    "```html\n",
    "pip install wtforms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This web app will prompt the user to enter their name, after pressing the button (**Say Hello**) the form will validate and render a new HTML page that will display the user's name.\n",
    "\n",
    "The new directory structure will look like this:\n",
    "```html\n",
    "flask_app_2/\n",
    "    app.py\n",
    "    static/\n",
    "        style.css\n",
    "    templates/\n",
    "        _formhelpers.html\n",
    "        first_app.html\n",
    "        hello.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's modify the app.py file:\n",
    "```python\n",
    "from flask import Flask, render_template, request\n",
    "from wtforms import Form, TextAreaField, validators\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class HelloForm(Form):\n",
    "    sayhello = TextAreaField('',[validators.DataRequired()])\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    form = HelloForm(request.form)\n",
    "    return render_template('first_app.html', form=form)\n",
    "\n",
    "@app.route('/hello', methods=['POST'])\n",
    "def hello():\n",
    "    form = HelloForm(request.form)\n",
    "    if request.method == 'POST' and form.validate():\n",
    "        name = request.form['sayhello']\n",
    "        return render_template('hello.html', name=name)\n",
    "    return render_template('first_app.html', form=form)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **wtforms**, we extended the **index()** function with a text field that we will embed in our start page using the _TextAreaField_ class, which automatically checks whether a user has provided valid input text or not.\n",
    "\n",
    "Then we defined a new function, **hello()**, which will render an HTML page **hello.html** if the form has been validated, and then we used the _POST_ method to transport the form data to the server in the message body.\n",
    "\n",
    "Finally we set the  argument _debug=True_ the activate Flask's debugger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **Jinja2** (http://jinja.pocoo.org) template engine, we will create a generic macro in **\\_formhelpers.html**, which we will later import in our **first_app.html** to render the text field. \n",
    "```html\n",
    "{% macro render_field(field) %}\n",
    "    <dt>{{ field.label }}\n",
    "    <dd>{{ field(**kwargs)|safe }}\n",
    "    {% if field.errors %}\n",
    "        <ul class=errors>\n",
    "        {% for error in field.errors %}\n",
    "            <li>{{ error }}</li>\n",
    "        {% endfor %}\n",
    "        </ul>\n",
    "    {% endif %}\n",
    "    </dd>\n",
    "{% endmacro %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside your static directory will modify the **style.css** file with this code:\n",
    "```CSS\n",
    "body {\n",
    "    font-size: 2em;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this code to the **first_app.html**:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>First app</title>\n",
    "        <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
    "    </head>\n",
    "    <body>\n",
    "        {% from \"_formhelpers.html\" import render_field %}\n",
    "        <div>What's your name?</div>\n",
    "        <form method=post action=\"/hello\">\n",
    "            <dl>\n",
    "                {{ render_field(form.sayhello) }}\n",
    "            </dl>\n",
    "            <input type=submit value='Say Hello' name='submit_btn'>\n",
    "        </form>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a **hello.html** file that will be rendered via the line return\n",
    "**render_template('hello.html', name=name)** inside the **hello()** function,\n",
    "which we defined in the _app.py_ script to display the text that a user submitted\n",
    "via the text field. The code is as follows:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>First app</title>\n",
    "        <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>Hello {{ name }}</div>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the flask_app_2 directory run the command below in your terminal or command prompt:\n",
    "```html\n",
    "python app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Classifier Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to implement the movie classifier into a Flask web application. \n",
    "\n",
    "#### Breakdown\n",
    "The web app will prompt the user to enter a movie review, after the review is submitted, the user will see a new page that shows the predicted class label and the probability of the prediction. Also the user will provide feedback about this prediction by clicking the _Correct_ or _Incorrect_ buttons and after clicking on the feedback buttons is a simple _thank you_ screen with a _Submit another review_ button that redirects the user back to the start page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the directory tree will look like:\n",
    "```html\n",
    "movieclassifier/\n",
    "    app.py\n",
    "    pkl_objects/\n",
    "        classifier.pkl\n",
    "        stopwords.pkl\n",
    "    reviews.sqlite\n",
    "    static/\n",
    "        style.css\n",
    "    templates/\n",
    "        _formhelpers.html\n",
    "        results.html\n",
    "        reviewform.html\n",
    "        thanks.html\n",
    "    vectorizer.py\n",
    "```\n",
    "You've already created this directory and the stuff that's inside it with earlier code when we created the movie classifier. We are just adding **app.py**, **static/**, and **templates**// inside the movieclassifier directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now modify the **app.py** file. Since this is a long piece of code, will break it into two parts:\n",
    "```python\n",
    "# Part one\n",
    "from flask import Flask, render_template, request\n",
    "from wtforms import Form, TextAreaField, validators\n",
    "import pickle\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# import HashingVectorizer from local dir\n",
    "from vectorizer import vect\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "######## Preparing the Classifier\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "clf = pickle.load(open(os.path.join(cur_dir, 'pkl_objects/classifier.pkl'), 'rb'))\n",
    "db = os.path.join(cur_dir, 'reviews.sqlite')\n",
    "\n",
    "def classify(document):\n",
    "    label = {0: 'negative', 1: 'positive'}\n",
    "    X = vect.transform([document])\n",
    "    y = clf.predict(X)[0]\n",
    "    proba = np.max(clf.predict_proba(X))\n",
    "    return label[y], proba\n",
    "\n",
    "def train(document, y):\n",
    "    X = vect.transform([document])\n",
    "    clf.partial_fit(X, [y])\n",
    "\n",
    "def sqlite_entry(path, document, y):\n",
    "    conn = sqlite3.connect(path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT INTO review_db (review, sentiment, date)\"\\\n",
    "    \" VALUES (?, ?, DATETIME('now'))\", (document, y))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Part two\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the **HashingVectorizer** and unpickled the logistic regression classifier. Next, we defined a **classify()** function to return the predicted class label as well as the corresponding probability prediction of a given text document. The **train()** function is used to update the classifier, as long as the document and a class label is provided. The **sqlite_entry** function can store a submitted movie review in the SQLite database with the its class label and timestamp. Note that the _clf_ object will be reset to its original, pickled state if we restart the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# Part two\n",
    "\n",
    "class ReviewForm(Form):\n",
    "    moviereview = TextAreaField('',\n",
    "                                [validators.DataRequired(),\n",
    "                                validators.length(min=15)])\n",
    "@app.route('/')\n",
    "def index():\n",
    "    form = ReviewForm(request.form)\n",
    "    return render_template('reviewform.html', form=form)\n",
    "\n",
    "@app.route('/results', methods=['POST'])\n",
    "def results():\n",
    "    form = ReviewForm(request.form)\n",
    "    if request.method == 'POST' and form.validate():\n",
    "        review = request.form['moviereview']\n",
    "        y, proba = classify(review)\n",
    "        return render_template('results.html',\n",
    "                                content=review,\n",
    "                                prediction=y,\n",
    "                                probability=round(proba*100, 2))\n",
    "    return render_template('reviewform.html', form=form)\n",
    "\n",
    "@app.route('/thanks', methods=['POST'])\n",
    "def feedback():\n",
    "    feedback = request.form['feedback_button']\n",
    "    review = request.form['review']\n",
    "    prediction = request.form['prediction']\n",
    "    \n",
    "    inv_label = {'negative': 0, 'positive': 1}\n",
    "    y = inv_label[prediction]\n",
    "    if feedback == 'Incorrect':\n",
    "        y = int(not(y))\n",
    "    train(review, y)\n",
    "    sqlite_entry(db, review, y)\n",
    "    return render_template('thanks.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We defined a **ReviewForm** class that instantiates a _TextAreaField_, which will be rendered in the **reviewform.html** template file (the landing page of our web app). This, in turn, is rendered by the **index()** function. With the **validators.length(min=15)** parameter, we require the user to enter a review that contains at least 15 characters.\n",
    "\n",
    "The **feedback()** function fetches the predicted class label from the **results.html** template if a user clicked on the _Correct_ or _Incorrect_ feedback button, and then transforms the predicted sentiment into an integer class label that will be used to update the classifier via **train()**. Also, a new entry to the SQLite database will be made an the **thanks.html** template will be rendered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create some HTML code for **reviewform.html**:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Movie Classification</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Please enter your movie review:</h2>\n",
    "        {% from \"_formhelpers.html\" import render_field %}\n",
    "        <form method=post action=\"/results\">\n",
    "            <dl>\n",
    "                {{ render_field(form.moviereview, cols='30', rows='10') }}\n",
    "            </dl>\n",
    "            <div>\n",
    "                <input type=submit value='Submit review' name='submit_btn'>\n",
    "            </div>\n",
    "        </form>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply imported the same **\\_formhelpers.html** template that we defined earlier. The **render_field()** function of this macro is used to render a _TextAreaField_ where a user can provide a movie review and submit it via the Submit review button displayed at the bottom of the page. This _TextAreaField_ is 30 columns wide and 10 rows tall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next template, **results.html**, looks a little bit more interesting:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Movie Classification</title>\n",
    "        <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3>Your movie review:</h3>\n",
    "        <div>{{ content }}</div>\n",
    "        <h3>Prediction:</h3>\n",
    "        <div>This movie review is <strong>{{ prediction }}</strong>\n",
    "            (probability: {{ probability }}%).</div>\n",
    "        <div id='button'>\n",
    "            <form action=\"/thanks\" method=\"post\">\n",
    "                <input type=submit value='Correct' name='feedback_button'>\n",
    "                <input type=submit value='Incorrect' name='feedback_button'>\n",
    "                <input type=hidden value='{{ prediction }}' name='prediction'>\n",
    "                <input type=hidden value='{{ content }}' name='review'>\n",
    "            </form>\n",
    "        </div>\n",
    "        <div id='button'>\n",
    "            <form action=\"/\">\n",
    "                <input type=submit value='Submit another review'>\n",
    "            </form>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we inserted the submitted review as well as the results of the prediction in the corresponding fields _{{ content }}_, _{{ prediction }}_, and _{{ probability }}_. You may notice that we used the _{{ content }}_ and _{{ prediction }}_ placeholder variables a second time in the form that contains the _Correct_ and _Incorrect_ buttons. This is a workaround to _POST_ those values back to the server to update the classifier and store the review in case the user clicks on one of those two buttons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now modify the **style.css** file in your **static/** directory:\n",
    "```CSS\n",
    "body{\n",
    "    width: 600px;\n",
    "}\n",
    "#button{\n",
    "    padding-top: 20px;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input this code into your **thanks.html** file in your **templates/** directory:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Movie Classification</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3>Thank you for your feedback!</h3>\n",
    "        <div id='button'>\n",
    "            <form action=\"/\">\n",
    "                <input type=submit value='Submit another review'>\n",
    "            </form>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
